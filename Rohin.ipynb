{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Cassandra data structures to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "session = cluster.connect('rohin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0xd88c708>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''prepare statement and create table cassandra'''\n",
    "\n",
    "\n",
    "statement = session.prepare(\"create table rohin.vechicle_group_M50 (cosit int, classname text, count int, stream_id int, primary key (cosit, classname, stream_id))\")\n",
    "session.execute(statement)\n",
    "\n",
    "statement = session.prepare(\"create table rohin.Average_velocity_M50 (cosit int, avg_speed float, stream_id int, primary key (cosit, stream_id))\")\n",
    "session.execute(statement)\n",
    "\n",
    "statement = session.prepare(\"create table rohin.Bussiest_Nodes_m50 (cosit int, count int, stream_id int, primary key (cosit,stream_id))\")\n",
    "session.execute(statement)\n",
    "\n",
    "statement = session.prepare(\"create table rohin.HGV_traffic_M50 (classname text, count int, stream_id int, primary key (classname,stream_id))\"  )\n",
    "session.execute(statement)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0xd8adac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' drop all tables created above'''\n",
    "session.execute(\"drop table if exists rohin.vechicle_group_M50\")\n",
    "session.execute(\"drop table if exists rohin.Average_velocity_M50\")\n",
    "session.execute(\"drop table if exists rohin.Bussiest_Nodes_m50\")\n",
    "session.execute(\"drop table if exists rohin.HGV_traffic_M50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Statement Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_Q1(cosit,classname,count,stream_id):\n",
    "    '''prepare statment and execute in cassandra'''\n",
    "    statement = session.prepare(\"insert into rohin.vechicle_group_M50 (cosit, classname, count, stream_id) values (?,?,?,?)\")\n",
    "    session.execute(statement, (cosit,classname,count,stream_id))\n",
    "def insert_Q2(cosit,avg_speed,stream_id):\n",
    "    statement = session.prepare(\"insert into rohin.Average_velocity_M50 (cosit, avg_speed, stream_id) values (?,?,?)\")\n",
    "    session.execute(statement, (cosit,avg_speed,stream_id))\n",
    "def insert_Q3(cosit,count,stream_id):\n",
    "    statement = session.prepare(\"insert into rohin.Bussiest_Nodes_m50 (cosit, count, stream_id) values (?,?,?)\")\n",
    "    session.execute(statement, (cosit,count,stream_id))\n",
    "def insert_Q4(classname,count,stream_id):\n",
    "    statement = session.prepare(\"insert into rohin.HGV_traffic_M50 (classname, count, stream_id) values (?,?,?)\")\n",
    "    session.execute(statement, (classname,count,stream_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select * from vechicle_group_M50\n",
    "# select * from Average_velocity_M50\n",
    "# select * from Bussiest_Nodes_m50\n",
    "# select * from HGV_traffic_M50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\pyspark\\context.py:227: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter/countdata0.csv\n",
      "0\n",
      "Start    01:59:13\n",
      "End  01:59:18\n",
      "Counter/countdata1.csv\n",
      "1\n",
      "Start    01:59:26\n",
      "End  01:59:31\n",
      "Counter/countdata2.csv\n",
      "3\n",
      "Start    01:59:42\n",
      "End  01:59:47\n",
      "Counter/countdata3.csv\n",
      "2\n",
      "Start    01:59:55\n",
      "End  02:00:00\n",
      "Counter/countdata4.csv\n",
      "0\n",
      "Start    02:00:04\n",
      "End  02:00:09\n",
      "Counter/countdata5.csv\n",
      "2\n",
      "Start    02:00:15\n",
      "End  02:00:20\n",
      "Counter/countdata6.csv\n",
      "2\n",
      "Start    02:00:25\n",
      "End  02:00:30\n",
      "Counter/countdata7.csv\n",
      "3\n",
      "Start    02:00:36\n",
      "End  02:00:41\n",
      "Counter/countdata8.csv\n",
      "1\n",
      "Start    02:00:46\n",
      "End  02:00:51\n",
      "Counter/countdata9.csv\n",
      "3\n",
      "Start    02:00:55\n",
      "End  02:01:00\n",
      "Counter/countdata10.csv\n",
      "1\n",
      "Start    02:01:05\n",
      "End  02:01:10\n",
      "Counter/countdata11.csv\n",
      "2\n",
      "Start    02:01:15\n",
      "End  02:01:20\n",
      "Counter/countdata12.csv\n",
      "4\n",
      "Start    02:01:29\n",
      "End  02:01:34\n",
      "Counter/countdata13.csv\n",
      "3\n",
      "Start    02:01:39\n",
      "End  02:01:44\n",
      "Counter/countdata14.csv\n",
      "0\n",
      "Start    02:01:48\n",
      "End  02:01:53\n",
      "Counter/countdata15.csv\n",
      "4\n",
      "Start    02:01:59\n",
      "End  02:02:04\n",
      "Counter/countdata16.csv\n",
      "3\n",
      "Start    02:02:14\n",
      "End  02:02:19\n",
      "Counter/countdata17.csv\n",
      "4\n",
      "Start    02:02:36\n",
      "End  02:02:41\n",
      "Counter/countdata18.csv\n",
      "1\n",
      "Start    02:02:49\n",
      "End  02:02:54\n",
      "Counter/countdata19.csv\n",
      "0\n",
      "Start    02:02:59\n",
      "End  02:03:04\n",
      "Counter/countdata20.csv\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Read CSV File into DataFrame').getOrCreate()\n",
    "stream_ID = 0\n",
    "import time\n",
    "while(True):\n",
    "    path = 'Counter/countdata'+str(stream_ID)+'.csv'\n",
    "    print(path)\n",
    "    if os.path.isfile(path):\n",
    "        Stream_lit_data = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        M50_nodes = [1012,1014,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,15010,15011,15012]\n",
    "        M50Datapoints = Stream_lit_data.filter(Stream_lit_data.cosit.isin(M50_nodes))\n",
    "        \n",
    "        vechicle_group_M50 = M50Datapoints.groupby('cosit','classname').count().withColumnRenamed('count','Vehicle_Frequency').withColumnRenamed('cosit','cosit_id').withColumnRenamed('classname','classname_id')\n",
    "        \n",
    "        for row in vechicle_group_M50.collect():\n",
    "            insert_Q1(row[0],row[1],row[2],stream_ID)\n",
    "            \n",
    "    \n",
    "        Average_velocity_M50 = M50Datapoints.groupby('cosit').agg({'speed':\"avg\"}).withColumnRenamed('avg(speed)','Mean_Velocity').withColumnRenamed('cosit','Node_ID')\n",
    "        \n",
    "        for row in Average_velocity_M50.collect():\n",
    "            insert_Q2(row[0],row[1],stream_ID)\n",
    "            \n",
    "            \n",
    "        Bussiest_Nodes_m50 = M50Datapoints.groupBy('cosit').count().sort('cosit',ascending=False).withColumnRenamed('count','Vehicle_Frequency').withColumnRenamed('cosit','Node_ID')\n",
    "        \n",
    "        for row in Bussiest_Nodes_m50.collect()[:3]:\n",
    "            insert_Q3(row[0],row[1],stream_ID)\n",
    "        print(len(Bussiest_Nodes_m50.collect()))  \n",
    "            \n",
    "        HGV_list = ['HGV_ART','HGV_RIG']\n",
    "        HGV_traffic_M50 = M50Datapoints.filter(M50Datapoints['classname'].isin(HGV_list)).groupby('classname').count()\n",
    "        \n",
    "        for row in HGV_traffic_M50.collect():\n",
    "            insert_Q4(row[0],row[1],stream_ID)\n",
    "            \n",
    "            \n",
    "        print('Start    '+str(time.strftime(\"%H:%M:%S\")))\n",
    "        time.sleep(5)\n",
    "        print('End  '+str(time.strftime(\"%H:%M:%S\")))\n",
    "        stream_ID += 1\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c3b02fa24688d32a1d09351b7f8eac82a87272ad081601c73caead00a8e1fc4"
  },
  "kernelspec": {
   "display_name": "Python 2.7.18 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
